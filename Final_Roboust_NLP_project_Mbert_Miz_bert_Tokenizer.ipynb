{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "id": "ka6MnyM1LHyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, BertModel, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import wandb"
      ],
      "metadata": {
        "id": "FHxd-zPULpJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "df1 = pd.read_excel(\"/content/mizo_sentiment_dataset.xlsx\")\n",
        "df2 = pd.read_excel(\"/content/Chungli_ao_Train.xlsx\")"
      ],
      "metadata": {
        "id": "quRcxxUrLryx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the datasets\n",
        "df = pd.concat([df1, df2])\n",
        "#df = df1"
      ],
      "metadata": {
        "id": "0he34t4ILx5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the dataframe\n",
        "df.info()"
      ],
      "metadata": {
        "id": "XEx3gNrQL0PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the labels to numbers\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['Sentiment'] = le.fit_transform(df.Sentiment.values)"
      ],
      "metadata": {
        "id": "HLLdbDjJL2bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataframe\n",
        "df = shuffle(df)"
      ],
      "metadata": {
        "id": "W8wPmP9rL4yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of positive and negative labels\n",
        "Positive_labels = (df['Sentiment'] == 1).sum()\n",
        "Negative_labels = (df['Sentiment'] == 0).sum()\n",
        "print(f\"Number of Postive labels: {Positive_labels}\")\n",
        "print(f\"Number of Negative labels: {Negative_labels}\")"
      ],
      "metadata": {
        "id": "2-1FRnXwL7Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels_str, val_labels_str = train_test_split(list(df['Text']), list(df['Sentiment']), test_size=.2)"
      ],
      "metadata": {
        "id": "cd596bc6L9K-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MizBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"robzchhangte/MizBERT\")"
      ],
      "metadata": {
        "id": "qPwtCXTLMAPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all elements are strings\n",
        "train_texts = [str(text) for text in train_texts]\n",
        "val_texts = [str(text) for text in val_texts]"
      ],
      "metadata": {
        "id": "cFEDeU16MDoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the texts with MizBERT tokenizer (truncation, padding, max_length of 512)\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)"
      ],
      "metadata": {
        "id": "wMckkSZ4MGO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "lo117m9mMIYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "train_labels = le.fit_transform(train_labels_str)\n",
        "val_labels = le.transform(val_labels_str)"
      ],
      "metadata": {
        "id": "9BhdJToCMK0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "K1ZLCrfvMNy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom classification model using mBERT (BertModel)\n",
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "        self.base_model = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)  # Removed token_type_ids\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use the representation of [CLS] token\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "2GK1RMzKMRyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the custom model with mBERT\n",
        "custom_model = CustomClassifier(model_name='bert-base-multilingual-cased', num_labels=2)"
      ],
      "metadata": {
        "id": "R17UI9OdMV6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'Accuracy': acc, 'F1': f1, 'Precision': precision, 'Recall': recall}"
      ],
      "metadata": {
        "id": "q3f6A9czMYU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "id": "2fPNpikxMcPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project='Roboust_nlp_mBERT', name='Chungliao+Mizo_mizbert_tokenizer_second_run')"
      ],
      "metadata": {
        "id": "U3XD8IC5MbeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "XNWEm7jUKqlZ",
        "outputId": "85e1d72c-b5ab-4350-9e13-baec473a2e7a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df1' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-7b2fa59892e9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Concatenate the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#df = df1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Display information about the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df1' is not defined"
          ]
        }
      ],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=0.0001,\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,   # batch size per device during training\n",
        "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.001,              # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    eval_steps=10,\n",
        "    report_to='wandb',\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=custom_model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # compute metrics function\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Error_analysis"
      ],
      "metadata": {
        "id": "kX_Uj9HDMpDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_excel(\"/content/Chungli_Ao_test.xlsx\")"
      ],
      "metadata": {
        "id": "ErFfaKvLMmRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df)"
      ],
      "metadata": {
        "id": "bqiHp8SsMt1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "Bj5CuQCFMwR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.tail()"
      ],
      "metadata": {
        "id": "6wh7bwiWMzoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = list(test_df['Text'])"
      ],
      "metadata": {
        "id": "IQqGpDEsM2vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['Sentiment'] = le.fit_transform(test_df.Sentiment.values)"
      ],
      "metadata": {
        "id": "5Gd-b4lFM8dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_df['Sentiment']"
      ],
      "metadata": {
        "id": "u832b-3iM9Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_labels)"
      ],
      "metadata": {
        "id": "9O_cfgjoM_yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "test_dataset =  CustomDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "KO7Rdu3jNCls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "h5JuhYrfNFw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_metrics)"
      ],
      "metadata": {
        "id": "vpICyFQRNJAe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}