{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoTokenizer, XLMRobertaModel, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import wandb"
      ],
      "metadata": {
        "id": "ws8l-phfO_fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the datasets\n",
        "df1 = pd.read_excel(\"/content/mizo_sentiment_dataset.xlsx\")\n",
        "df2 = pd.read_excel(\"/content/Chungli_ao_Train.xlsx\")"
      ],
      "metadata": {
        "id": "_GN6ihbgPDZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df = df1\n",
        "df = pd.concat([df1, df2])"
      ],
      "metadata": {
        "id": "tK4fsNVbPGNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display information about the dataframe\n",
        "df.info()"
      ],
      "metadata": {
        "id": "mXek9b8NPG8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the labels to numbers\n",
        "le = preprocessing.LabelEncoder()\n",
        "df['Sentiment'] = le.fit_transform(df.Sentiment.values)"
      ],
      "metadata": {
        "id": "YB8uPbUIPJDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataframe\n",
        "df = shuffle(df)"
      ],
      "metadata": {
        "id": "KERAhzKXPMpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of positive and negative labels\n",
        "Positive_labels = (df['Sentiment'] == 1).sum()\n",
        "Negative_labels = (df['Sentiment'] == 0).sum()\n",
        "print(f\"Number of Postive labels: {Positive_labels}\")\n",
        "print(f\"Number of Negative labels: {Negative_labels}\")"
      ],
      "metadata": {
        "id": "tLji7vkhPOby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "train_texts, val_texts, train_labels_str, val_labels_str = train_test_split(list(df['Text']), list(df['Sentiment']), test_size=.2)"
      ],
      "metadata": {
        "id": "E9_DKyZcPQoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MizBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"robzchhangte/MizBERT\")"
      ],
      "metadata": {
        "id": "pMSKcMGbPVpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load XLM-Roberta model\n",
        "model_name = 'xlm-roberta-base'\n",
        "model = XLMRobertaModel.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "AtYxo3w6PZpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure all elements are strings\n",
        "train_texts = [str(text) for text in train_texts]\n",
        "val_texts = [str(text) for text in val_texts]"
      ],
      "metadata": {
        "id": "zQzC-lUyPaZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the texts with MizBERT tokenizer (truncation, padding, max_length of 512)\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)"
      ],
      "metadata": {
        "id": "D6Q7yg3WPeRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom dataset class\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ],
      "metadata": {
        "id": "DfDVOaRNPgK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode labels\n",
        "train_labels = le.fit_transform(train_labels_str)\n",
        "val_labels = le.transform(val_labels_str)"
      ],
      "metadata": {
        "id": "LzZYk7mZPiQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset objects\n",
        "train_dataset = CustomDataset(train_encodings, train_labels)\n",
        "val_dataset = CustomDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "Ox-fXkrjPkTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define custom classification model using XLM-Roberta model\n",
        "class CustomClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels):\n",
        "        super(CustomClassifier, self).__init__()\n",
        "        self.base_model = XLMRobertaModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)  # Removed token_type_ids\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # Use the representation of [CLS] token\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, 2), labels.view(-1))\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits"
      ],
      "metadata": {
        "id": "h4ef99zwPmsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the custom model with XLM-Roberta base\n",
        "custom_model = CustomClassifier(model_name='xlm-roberta-base', num_labels=2)"
      ],
      "metadata": {
        "id": "9jnaK_VYPp8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define metrics\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='micro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {'Accuracy': acc, 'F1': f1, 'Precision': precision, 'Recall': recall}"
      ],
      "metadata": {
        "id": "9zcUp60JPsOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb"
      ],
      "metadata": {
        "id": "MY0UAXdWPwrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize wandb\n",
        "wandb.init(project='Roboust_nlp_XLMR', name='Chungliao+Mizo_mizbert_tokenizer_second_run')"
      ],
      "metadata": {
        "id": "arsyXF2SPujb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BO4-KDcJ8Hw"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=0.0001,\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=16,   # batch size per device during training\n",
        "    per_device_eval_batch_size=16,    # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.001,              # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    do_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    eval_steps=10,\n",
        "    report_to='wandb',\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=custom_model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset,            # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # compute metrics function\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0pvJ8yRzP9dZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "AFx1oC79P5zT",
        "outputId": "2fea9b80-69bb-4da8-f669-a73a335f9c78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f633aefc008c>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test_set"
      ],
      "metadata": {
        "id": "ulF7yAJrQQnN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_df = pd.read_excel(\"/content/Chungli_Ao_test.xlsx\")"
      ],
      "metadata": {
        "id": "WrzM3CrjQVT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_df)"
      ],
      "metadata": {
        "id": "c_X2lcbEQZSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "id": "e1xItk8cQnve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.tail()"
      ],
      "metadata": {
        "id": "1XQbj13xQoko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = list(test_df['Text'])"
      ],
      "metadata": {
        "id": "08xy9cX0QrPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['Sentiment'] = le.fit_transform(test_df.Sentiment.values)"
      ],
      "metadata": {
        "id": "goHbc-FGQt6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = test_df['Sentiment']"
      ],
      "metadata": {
        "id": "3DdDXIe_QwqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_labels)"
      ],
      "metadata": {
        "id": "FD_16Aq5Qy8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
        "test_dataset = CustomDataset(test_encodings, test_labels)"
      ],
      "metadata": {
        "id": "0NlUQHYpQ4ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_metrics = trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "Xqfgrzi1Q5EE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_metrics)"
      ],
      "metadata": {
        "id": "UaXhTWtIQ7fY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}