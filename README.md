This project was part of a robust NLP seminar course. In this joint project with Toshiki, I focused on fine-tuning XLM-R and m-BERT models on Chungli Ao and Mizo data, and testing on Chungli Ao (Task 1). I also worked on Adapter Fusion between Chungli Ao and Mizo (Task 3).

The project aimed to answer two main questions:

    How effective are existing multilingual pre-trained models and their tokenizers for Chungli Ao?
    How does adapter fusion compare to standard fine-tuning?

The results showed that adapter fusion performed better than standard fine-tuning, proving its efficiency in low-resource settings. Additionally, we created sentiment analysis adapters for Mizo and Chungli Ao and explored how well current multilingual pre-trained models work for Chungli Ao.
